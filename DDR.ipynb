{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, utils\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from torchcam.methods import GradCAM\n",
    "from torchcam.utils import overlay_mask\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, size=(224, 224)):\n",
    "    # Open and resize image\n",
    "    # image = Image.open(image_path)\n",
    "    image = image.resize(size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Convert the image to a numpy array and then to grayscale\n",
    "    img = np.array(image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply a binary threshold to get a binary image\n",
    "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "   # Check if contours list is not empty\n",
    "    if contours:\n",
    "        # Find the largest contour which should be the fundus\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Get the bounding rectangle for the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        # Crop the image to the bounding rectangle\n",
    "        cropped = image.crop((x, y, x+w, y+h))\n",
    "    else:\n",
    "        print(f\"No contours found in image\")\n",
    "        cropped = image  # or handle this case differently as per your requirement\n",
    "\n",
    "    # Resize the cropped image to the desired size\n",
    "    cropped = cropped.resize(size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # # Color normalization\n",
    "    # mean = np.mean(cropped, axis=(0, 1))\n",
    "    # image_normalized = cropped - mean\n",
    "\n",
    "    # Illumination correction and contrast enhancement using CLAHE\n",
    "    image_lab = cv2.cvtColor(np.uint8(cropped), cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(image_lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "    image_clahe = cv2.merge((l_clahe, a, b))\n",
    "    image_clahe_rgb = cv2.cvtColor(image_clahe, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "\n",
    "    # Convert the image to uint8 before applying Gaussian blur\n",
    "    image_clahe_rgb_uint8 = (image_clahe_rgb * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    image_denoised = cv2.GaussianBlur(image_clahe_rgb_uint8, (5, 5), 0.5)\n",
    "\n",
    "    image_denoised = Image.fromarray(image_denoised)\n",
    "    return image_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_fundus_image(image_path, save_path=None):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply a binary threshold to get a binary image\n",
    "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Find the largest contour which should be the fundus\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Get the bounding rectangle for the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # Crop the image to the bounding rectangle\n",
    "    cropped_img = img[y:y+h, x:x+w]\n",
    "    \n",
    "    # Save or return the cropped image\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, cropped_img)\n",
    "    else:\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_images(dataset_dirs=['DDR-dataset/DR_grading/train', 'DDR-dataset/DR_grading/valid', 'DDR-dataset/DR_grading/test']):\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        image_files = os.listdir(dataset_dir)\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(dataset_dir, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            preprocessed_image = preprocess_image(image)\n",
    "            save_path = os.path.join(dataset_dir + '_preprocessed', image_file)\n",
    "            preprocessed_image.save(save_path)\n",
    "\n",
    "# preprocess_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('DDR-dataset/DR_grading/train.txt', sep=' ', header=None, names=['image', 'label'])\n",
    "train_df = train_df.query('label != 5')\n",
    "train_df['path'] = train_df['image'].apply(lambda x: os.path.join('DDR-dataset/DR_grading/train_preprocessed', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('DDR-dataset/DR_grading/valid.txt', sep=' ', header=None, names=['image', 'label'])\n",
    "val_df = val_df.query('label != 5')\n",
    "val_df['path'] = val_df['image'].apply(lambda x: os.path.join('DDR-dataset/DR_grading/valid_preprocessed', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    new_rows = []\n",
    "    class_counts = np.bincount(df['label'])\n",
    "    difference = np.max(class_counts) - class_counts\n",
    "    aug_transforms = v2.Compose([\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.RandomVerticalFlip(p=0.5),\n",
    "        v2.RandomRotation(degrees=360),\n",
    "        v2.RandomPerspective(distortion_scale=0, p=1, interpolation=3),\n",
    "        v2.RandomAffine(degrees=0, translate=(0, 0), scale=(1, 1), shear=0),\n",
    "        v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0, hue=0),\n",
    "    ])\n",
    "    for i, count in enumerate(difference):\n",
    "        if count > 0:\n",
    "            samples = df.query(f'label == {i}').sample(count, replace=True)\n",
    "            # print(len(samples))\n",
    "            for index, row in samples.iterrows():\n",
    "                image = Image.open(row['path'])\n",
    "                augmented_image = aug_transforms(image)\n",
    "                augmented_image.save(f'{row[\"path\"][:-4]}_augmented{count}.png')\n",
    "                new_rows.append({'image': f'{row[\"image\"][:-4]}_augmented{count}.png', 'label': i, 'path': f'{row[\"path\"][:-4]}_augmented{count}.png'})\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    new_df = pd.concat([df, new_df])\n",
    "    return new_df\n",
    "\n",
    "# new_df = balance_dataset(train_df)\n",
    "\n",
    "# new_df.to_csv('DDR-dataset/DR_grading/train_augmented.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('DDR-dataset/DR_grading/train_augmented.txt', sep=' ', header=None, names=['image', 'label', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['path']\n",
    "y_train = train_df['label']\n",
    "x_val = val_df['path']\n",
    "y_val = val_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = train_df['path']\n",
    "\n",
    "# y_train = pd.get_dummies(train_df['label']).values\n",
    "\n",
    "# y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n",
    "# y_train_multi[:, 4] = y_train[:, 4]\n",
    "\n",
    "# for i in range(3, -1, -1):\n",
    "#     y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n",
    "\n",
    "# print(\"Original y_train:\", y_train.sum(axis=0))\n",
    "# print(\"Multilabel version:\", y_train_multi.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_val = val_df['path']\n",
    "# y_val = pd.get_dummies(val_df['label']).values\n",
    "# y_val_multi = np.empty(y_val.shape, dtype=y_val.dtype)\n",
    "# y_val_multi[:, 4] = y_val[:, 4]\n",
    "\n",
    "# for i in range(3, -1, -1):\n",
    "#     y_val_multi[:, i] = np.logical_or(y_val[:, i], y_val_multi[:, i+1])\n",
    "\n",
    "# print(\"Original y_val:\", y_val.sum(axis=0))\n",
    "# print(\"Multilabel version:\", y_val_multi.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, paths, label, transform=None):\n",
    "        self.paths = paths\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths.iloc[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define data augmentation transformations\n",
    "aug_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=360),\n",
    "    v2.RandomPerspective(distortion_scale=0, p=1, interpolation=3),\n",
    "    v2.RandomAffine(degrees=0, translate=(0, 0), scale=(1, 1), shear=0),\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0, hue=0),\n",
    "])\n",
    "\n",
    "# Define the main transformation pipeline\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.RandomApply([aug_transforms], p=0.5),  # Apply augmentations with 50% probability\n",
    "    # v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create custom datasets and dataloaders for training and validation\n",
    "train_dataset = CustomImageDataset(x_train,y_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(x_val,y_val, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_batch(batch):\n",
    "    images, labels = batch\n",
    "    grid_img = utils.make_grid(images, nrow=8)\n",
    "    grid_img = np.transpose(grid_img, (1, 2, 0))\n",
    "    # print(images.size())\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over the train_dataloader and visualize a batch\n",
    "for batch in train_dataloader:\n",
    "    show_batch(batch)\n",
    "    break  # Stop after visualizing the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "Epoch [1/1], Loss: 0.0452, Val Loss: 3.6316, QWK: 0.0000, Acc: 0.5006\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet-50\n",
    "resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "# Modify the final fully connected layer for your specific classification task\n",
    "num_classes = 5  # Assuming you have 5 classes\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "   \n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "resnet.to(device)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet(inputs)\n",
    "            val_loss += loss_func(outputs, labels).item()\n",
    "\n",
    "            all_preds.append(outputs.cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "        \n",
    "    # Flatten predictions and labels for Quadratic Weighted Kappa calculation\n",
    "    all_preds = np.concatenate(all_preds).argmax(axis=1)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "\n",
    "    # Calculate Quadratic Weighted Kappa\n",
    "    qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print('saving model')\n",
    "        torch.save(resnet.state_dict(), 'DDRresnet50_best_acc.pth')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, QWK: {qwk:.4f}, Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test image\n",
    "test_image_path = 'train_images/0024cdab0c1e.png'\n",
    "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the transformation pipeline for test images\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformations to the test image\n",
    "test_image = test_transform(test_image)\n",
    "\n",
    "# If you're using GPU, move the image to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "test_image = test_image.to(device)\n",
    "\n",
    "# Load the trained model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_classes = 5  # Assuming you have 5 classes\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(resnet.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "resnet.load_state_dict(torch.load('ResNet50.pth', map_location=device))\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = resnet(test_image.unsqueeze(0))  # Add a batch dimension\n",
    "\n",
    "# Convert the output probabilities to predicted labels\n",
    "predicted_labels = np.round(outputs.cpu().numpy()).astype(int)\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = 'train_images/0024cdab0c1e.png'\n",
    "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "\n",
    "preprocess = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "original_image_preprocess = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "test_image_tensor = preprocess(test_image)\n",
    "original_image_tensor = original_image_preprocess(test_image)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "test_image_tensor = test_image_tensor.to(device)\n",
    "original_image_tensor = original_image_tensor.to(device)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_classes = 5 # Number of predicted classes\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.load_state_dict(torch.load('ResNet50.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "last_conv_layer = 'layer4'\n",
    "cam_extractor = GradCAM(model, last_conv_layer)\n",
    "out = model(test_image_tensor.unsqueeze(0)) \n",
    "predicted_class = int(np.round(out.cpu().detach().numpy()).astype(int).sum() - 1)\n",
    "cams = cam_extractor(predicted_class, out)\n",
    "\n",
    "\n",
    "result = overlay_mask(to_pil_image(original_image_tensor), to_pil_image(cams[0].squeeze(0), mode='F'), alpha=0.5) \n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_label_and_gradcam(image, model, last_conv_layer):\n",
    "    preprocess = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    original_image_preprocess = v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    image = image.convert(\"RGB\")\n",
    "    test_image_tensor = preprocess(image)\n",
    "    original_image_tensor = original_image_preprocess(image)\n",
    "\n",
    "    test_image_tensor = test_image_tensor.to(device)\n",
    "    original_image_tensor = original_image_tensor.to(device)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    cam_extractor = GradCAM(model, last_conv_layer)\n",
    "    out = model(test_image_tensor.unsqueeze(0)) \n",
    "    predicted_class = int(np.round(out.cpu().detach().numpy()).astype(int).sum() - 1)\n",
    "    cams = cam_extractor(predicted_class, out)\n",
    "\n",
    "    # resized_gradcam = torch.nn.functional.interpolate(cams[0].unsqueeze(0), size=original_image_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    # gradcam_image = overlay_mask(to_pil_image(original_image_tensor), to_pil_image(resized_gradcam.squeeze(0), mode='F'), alpha=0.5)\n",
    "    gradcam_image = overlay_mask(to_pil_image(original_image_tensor), to_pil_image(cams[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "\n",
    "    return predicted_class, gradcam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "test_image_path = 'DDR-dataset/DR_grading/train_preprocessed/007-0004-000.jpg'\n",
    "image = Image.open(test_image_path)\n",
    "\n",
    "model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "num_classes = 5 # Number of predicted classes\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.load_state_dict(torch.load('ResNet50.pth', map_location=torch.device(\"mps\")))\n",
    "\n",
    "last_conv_layer = 'layer4'\n",
    "\n",
    "predicted_label, gradcam_images = get_predicted_label_and_gradcam(image, model, last_conv_layer)\n",
    "\n",
    "print(\"Predicted Label:\", predicted_label)\n",
    "plt.imshow(gradcam_images)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
